{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Recommendation System in PySpark \n",
    "# Stage 1: Data Collection and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "* Demonsrate an understanding on how recommendation systems are being used for personalization of online services/products.\n",
    "* Download, unarchive and store large datasets from within the PySpark computational environment.\n",
    "* Parse and filter datasets into Spark data structures, performing basic feature selection. \n",
    "* Run a hyper-parameter selction activity through a scalable grid search. \n",
    "* Train and evaluate the predictive performance of recommendation system.\n",
    "* Generate predictions from the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "Recommender/Recommendation Systems are one of the most successful applications of machine learning in the Big Data domain. Such systems are integral parts in the success of Amazon (Books, Items), Pandora/Spotify (Music), Google (News, Search), YouTube (Videos) etc.  For Amazon these systems bring more than 30% of their total revenues. For Netflix service, 75% of movies that people watch are based on some sort of recommendation.\n",
    "\n",
    "> The goal of Recommendation Systems is to find what is likely to be of interest to the user. This enables organizations to offer a high level of personalization and customer tailored services.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*Zvwzw_KPRv5bcXPkb6WubA.jpeg)\n",
    "\n",
    "For online video content services like Netflix and Hulu, the need to build robust movie recommendation systems is extremely important. An example of recommendation system is such as this:\n",
    "\n",
    "    User A watches Game of Thrones and Breaking Bad.\n",
    "    User B performs a search query for Game of Thrones.\n",
    "    The system suggests Breaking Bad to user B from data collected about user A.\n",
    "    \n",
    "\n",
    "\n",
    "This lab will guide you through a step-by-step process into developing such a movie recommendation system. We shall use the MovieLens dataset to build a movie recommendation system using collaborative filtering technique with Spark's Alternating Least Saqures implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovieLens Ratings Dataset\n",
    "\n",
    "Social computing research centre at university of Minnesota, [GroupLens Research](https://grouplens.org/),  has developed a movie ratings dataset called the [MovieLens](http://movielens.org/). The datasets were collected over various periods of time and can be directly downloaded from [this location](http://grouplens.org/datasets/movielens/). \n",
    "\n",
    "A data dictionary with details on individual datasets and included features can be viewed [HERE](http://files.grouplens.org/datasets/movielens/ml-20m-README.html)\n",
    "\n",
    "For our experiment , we shall download the latest datasets direct from the website in the zip format. Grouplens offer the complete ratings dataset and a small subset for experimentation. We shall down both these datasets for building our recommendation system. \n",
    "\n",
    "* **Small Dataset**: 100,000 ratings applied to 9,000 movies by 700 users. Last updated 10/2016.\n",
    "\n",
    "* **Complete Dataset**: 26,000,000 ratings applied to 45,000 movies by 270,000 users. Last updated 8/2017.\n",
    "\n",
    "\n",
    "Let's first set the URLs for both datasets for downloading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallURL = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "completeURL = 'http://files.grouplens.org/datasets/movielens/ml-latest.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also import PySpark to our Python environment and and initiate a local SparkContext `sc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]') # [*] represents a local context i.e. no cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Path \n",
    "\n",
    "In order to understand data processing on a cluster platform using very large datasets, which may be very hard and time consuming to download and store manually, we shall download, unzip and store the data from within Pyspark environment. \n",
    "\n",
    "Let's define the download locations and filenames for both datasets. \n",
    "\n",
    "* Create a folder \"datasets\" to download and save the zip files using [os.mkdir(path)](https://www.tutorialspoint.com/python/os_mkdir.htm). \n",
    "\n",
    "* Define paths for individual zip files for complete and small datasets. This can be achieved using [os.path.join()](https://code-maven.com/slides/python-programming/os-path-join) method. \n",
    "\n",
    "You need to import the `os` module into Python which provides directory system access, and save the zip files under their names as **ml-latest.zip** and **ml-latest-small.zip**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9576c5445ac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# create a directory \"datasets\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetsPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdatasetsPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'datasets'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "datasetsPath = \"datasets\"\n",
    "\n",
    "# create a directory \"datasets\"\n",
    "os.mkdir(datasetsPath)\n",
    "datasetsPath = os.path.join(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create path variables with filenames for both datasets\n",
    "completePath = os.path.join(datasetsPath, 'ml-latest.zip')\n",
    "smallPath = os.path.join(datasetsPath, 'ml-latest-small.zip')\n",
    "\n",
    "# datasets/ml-latest-small.zip\n",
    "# datasets/ml-latest.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Datasets\n",
    "We can now move on and download both files from the server using URL provided above. \n",
    "\n",
    "Import `urllib.request` module in Python and use `urllib.request.urlretrieve(url, path)` with urls and paths given above for both datasets. A good resource on downloading files in python can be found [HERE](http://stackabuse.com/download-files-with-python/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "#Download the datasets, provide URL and local destination for each dataset\n",
    "small = urllib.request.urlretrieve(smallURL, smallPath)\n",
    "complete= urllib.request.urlretrieve(completeURL, completePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Datasets to Folders \n",
    "\n",
    "We need to unzip the small and full datasets into their own respective folders. Let's use Python's `zipfile` module to perform this task. Details on the `zipfile` module can be seen from [THIS](https://pymotw.com/2/zipfile/) resource.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Unzip and Save complete data and the subset in their respective paths using zipfile module\n",
    "with zipfile.ZipFile(smallPath, \"r\") as z:\n",
    "    z.extractall(datasetsPath)\n",
    "\n",
    "with zipfile.ZipFile(completePath, \"r\") as z:\n",
    "    z.extractall(datasetsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above actions will generate following file and directory structure:\n",
    "\n",
    "![](path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Parsing, Selection and Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our SparkContext initialized, and datasets in their respective locations, we can now parse these csv files and read them into RDDs as shown in the previous labs. The small dataset contains a number of csv file as shown below:  \n",
    "\n",
    "#### ratings.csv\n",
    "Each line in the ratings dataset (ratings.csv) is formatted as:**userId, movieId, rating, timestamp**\n",
    "\n",
    "#### movies.csv\n",
    "Each line in the movies (movies.csv) dataset is formatted as:**movieId, title, genres**\n",
    "\n",
    "Genres has the format:**Genre1|Genre2|Genre3...**\n",
    "\n",
    "#### tags.csv\n",
    "The tags file (tags.csv) has the format:**userId, movieId, tag, timestamp**\n",
    "\n",
    "#### links.csv\n",
    "Contains links to IMDB and has the format:**movieId, imdbId, tmdbId**\n",
    "\n",
    "The complete dataset contains some other files as well which are not needed for this experiment. We shall focus on ratings.csv, and movies.csv for building a basic recommendation system. Other features can be incorporated later for improving the predictive performance of the system. \n",
    "\n",
    "The format of these files is uniform and simple and such comma delimited files can be parsed line by line using Python `split()`  once they are loaded into RDDs. \n",
    "\n",
    "Let's first work with the small dataset to get parsing code in place. We shall first parse ratings and movies files into two RDDs. We also need to filter out the header row  in each file. \n",
    "\n",
    "> **For each line in the ratings dataset, we create a tuple of (UserID, MovieID, Rating). We drop the timestamp because we do not need it for this recommender. **\n",
    "\n",
    "> **For each line in the movies dataset, we create a tuple of (MovieID, Title). We drop the genres because we do not use them for this recommender.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/ml-latest-small/ratings.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a path for identifying the ratings file \n",
    "\n",
    "smallRatingsPath = os.path.join(datasetsPath, 'ml-latest-small', 'ratings.csv')\n",
    "smallRatingsPath\n",
    "\n",
    "# 'datasets/ml-latest-small/ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'userId,movieId,rating,timestamp'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use .textFile() to read the raw contents of ratings file into an RDD\n",
    "# read the first line of this RDD as a header and view header contents\n",
    "\n",
    "smallRatingsRaw = sc.textFile(smallRatingsPath)\n",
    "smallRatingsHeader = smallRatingsRaw.take(1)[0]\n",
    "smallRatingsHeader\n",
    "\n",
    "# 'userId,movieId,rating,timestamp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can parse the raw data into a new RDD. Perform following transformations on `smallRatingsRaw`:\n",
    "\n",
    "1. Use `.filter()` to exclude the header information collected above\n",
    "2. Split each line of the csv file using `,` as the input argument with `split()` function\n",
    "3. Collect the first three elements of each row (UserID, MovieID, Rating) and discard timestep field.\n",
    "4. Cache the final RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallRatingsNoHeader= smallRatingsRaw.filter(lambda line: line != smallRatingsHeader )\n",
    "smallRatingsSplit = smallRatingsNoHeader.map(lambda line: line.split(\",\"))\n",
    "smallRatingsRDD = smallRatingsSplit.map(lambda tokens: (tokens[0],tokens[1],tokens[2]))\n",
    "smallRatingsRDD.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '31', '2.5'),\n",
       " ('1', '1029', '3.0'),\n",
       " ('1', '1061', '3.0'),\n",
       " ('1', '1129', '2.0'),\n",
       " ('1', '1172', '4.0')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first five rows of the final RDD\n",
    "smallRatingsRDD.take(5)\n",
    "\n",
    "# [('1', '31', '2.5'),\n",
    "#  ('1', '1029', '3.0'),\n",
    "#  ('1', '1061', '3.0'),\n",
    "#  ('1', '1129', '2.0'),\n",
    "#  ('1', '1172', '4.0')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now proceed in a similar way with movies.csv file. Repeat following steps as performed above:\n",
    "1. Create a path variable identifying the location of **movies.csv**\n",
    "2. Read the text file into RDD\n",
    "3. Exclude the header information\n",
    "4. Split the line contents of the csv file\n",
    "5. Read the contents of resulting RDD creating a (MovieID, Title) tuple and discard genres. \n",
    "6. Take 5 elements for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'Toy Story (1995)'),\n",
       " ('2', 'Jumanji (1995)'),\n",
       " ('3', 'Grumpier Old Men (1995)'),\n",
       " ('4', 'Waiting to Exhale (1995)'),\n",
       " ('5', 'Father of the Bride Part II (1995)')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallMoviesPath = os.path.join(datasetsPath, 'ml-latest-small', 'movies.csv')\n",
    "\n",
    "smallMoviesRaw = sc.textFile(smallMoviesPath)\n",
    "smallMoviesHeader = smallMoviesRaw.take(1)[0]\n",
    "\n",
    "smallMoviesRDD = smallMoviesRaw.filter(lambda line: line != smallMoviesHeader)\\\n",
    "                               .map(lambda line: line.split(\",\"))\\\n",
    "                               .map(lambda tokens: (tokens[0],tokens[1]))\\\n",
    "                               .cache()\n",
    "smallMoviesRDD.take(5)\n",
    "\n",
    "# [('1', 'Toy Story (1995)'),\n",
    "#  ('2', 'Jumanji (1995)'),\n",
    "#  ('3', 'Grumpier Old Men (1995)'),\n",
    "#  ('4', 'Waiting to Exhale (1995)'),\n",
    "#  ('5', 'Father of the Bride Part II (1995)')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Pre-Processed Data (optional)\n",
    "\n",
    "We can optionally save our preprocessed datasets. Create a folder \"processed\" and save `smallMovieRDD` and `smallRatingsRDD` using `RDD.saveAsTExtFile(location, filename)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory \"processed\" and store the preprocessed dataset RDDs as text files using .saveAsTExtFiles() method. \n",
    "\n",
    "processedPath = 'processed'\n",
    "os.mkdir(processedPath)\n",
    "\n",
    "smallMoviesRDD.saveAsTextFile(os.path.join(processedPath, 'smallMoviesRDD'))\n",
    "smallRatingsRDD.saveAsTextFile(os.path.join(processedPath, 'smallRatingsRDD'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Alternate Least Squares: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data as Testing , Training and Validation Sets. \n",
    "\n",
    "We can now go ahead and split the data for validating recommendation system. We can use spark's `randomsplit()` transformation that uses given weights to split an rdd into any number of sub-RDDs. The standared usage of this transformation function is :\n",
    "> `RDD.randomSplit(weights, seed=None)`\n",
    "\n",
    "**weights** – weights for splits, will be normalized if they don’t sum to 1\n",
    "\n",
    "**seed** – random seed\n",
    "\n",
    "Remember that the exact number of entries in each dataset varies slightly due to the random nature of the `randomSplit()` transformation.\n",
    "\n",
    "Let's split the smallRatingsRDD into training, testing and validation RDDs (60%, 20%, 20%) using respective weights.\n",
    "Perform a `.count` on resulting datasets to view the count of elements of each RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59907, 19993, 20104)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split ratingsRDD into training, validation and testing RDDs \n",
    "# Set seed to 100 for reproducibility\n",
    "\n",
    "trainRDD, validRDD, testRDD = smallRatingsRDD.randomSplit([6, 2, 2], seed=100)\n",
    "\n",
    "trainRDD.count(), testRDD.count(), validRDD.count()\n",
    "\n",
    "# (59907, 19993, 20104) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prediction of ratings, we would need `customerID` and `movieID` from validation and test RDDs respectively. Let's map these values into two new RDDs which will be used for training and validation purpose. We shall ignore the ratings values for these RDDs, as these will be predicted later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('1', '31'), ('1', '1061'), ('1', '1129')],\n",
       " [('1', '1287'), ('1', '1293'), ('1', '1339')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read customer ID and movie ID from validation and test sets. DO NOT show ground truth (ratings) to the model \n",
    "\n",
    "validPredictionRDD = validRDD.map(lambda x: (x[0], x[1]))\n",
    "testPredictionRDD = testRDD.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "validPredictionRDD.take(3), testPredictionRDD.take(3)\n",
    "\n",
    "# ([('1', '31'), ('1', '1061'), ('1', '1129')],\n",
    "#  [('1', '1287'), ('1', '1293'), ('1', '1339')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering\n",
    "\n",
    "Collaborative filtering allows us to make predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). \n",
    "\n",
    "The underlying assumption is that if a user A has the same opinion as a user B on an issue, A is more likely to have B's opinion on a different issue x than to have the opinion on x of a user chosen randomly. Following gif (from [Wikipedia](https://en.wikipedia.org/wiki/Collaborative_filtering)) shows an example of collaborative filtering. At first, people rate different items (like videos, images, games). Then, the system makes predictions about a user's rating for an item not rated yet. The new predictions are built upon the existing ratings of other users with similar ratings with the active user. In the image, the system predicts that the user will not like the video.\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/5/52/Collaborative_filtering.gif)\n",
    "\n",
    "\n",
    "Spark MLlib library for Machine Learning provides a Collaborative Filtering implementation by using Alternating Least Squares (ALS) lgorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Least Squares in Spark\n",
    "\n",
    "The ALS implementation for collaborative filtering in MLlib has the following parameters:\n",
    "\n",
    "* `numBlocks` is the number of blocks used to parallelize computation **(default: set to -1 to auto-configure)**\n",
    "* `rank` is the number of latent factors in the model. **(use the list [2,4,6,8,10] as rank values)**\n",
    "* `iterations` is the number of iterations to run. **(set to 15)**\n",
    "* `lambda` specifies the regularization parameter in ALS.**(set to 0.1)**\n",
    "* `implicitPrefs` specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data.**(use default)**\n",
    "* `alpha` is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations.**(use default)**\n",
    "\n",
    "Details on spark's ALS implementation can be viewed [HERE](https://spark.apache.org/docs/2.2.0/mllib-collaborative-filtering.html)\n",
    "\n",
    "\n",
    "\n",
    "We shall now import the ALS algorithm rom spark's machine learning library `mllib` and set the learning and training parameter values as shown above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Validation for hyper-parameter optimization\n",
    "\n",
    "We can now start our training process using above parameter values which would include following steps: \n",
    "\n",
    "* Run the training for each of the rank values in our `ranks` list inside a for loop.\n",
    "* Train the model using trainRDD, rank, seed, iterations and lambda value as model parameters. \n",
    "* Validate the trained model by predicting ratings for `validPredictionRDD`.\n",
    "* Compare predicted ratings to actual ratings. \n",
    "* calculate error as RMSE for each rank. \n",
    "\n",
    "For sake of simplicity, we shall repeat training process for changing ranks value **only**. Other values can also be changed as a detailed grid search for improved predictive performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 2 the RMSE is 0.9420055149124977\n",
      "For rank 4 the RMSE is 0.9336033590596436\n",
      "For rank 6 the RMSE is 0.9481011551989256\n",
      "For rank 8 the RMSE is 0.9447440477995621\n",
      "For rank 10 the RMSE is 0.945496424548251\n",
      "The best model was trained with rank 4\n"
     ]
    }
   ],
   "source": [
    "# Import ALS from spark's mllib\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "\n",
    "# set learning parameters \n",
    "seed = 500\n",
    "iterations = 10\n",
    "lambdaRegParam = 0.1\n",
    "ranks = [2, 4, 6, 8, 10]\n",
    "errors = [0, 0, 0, 0, 0] # initialize a matrix for storing error values\n",
    "err = 0 # iterator for above list \n",
    "\n",
    "\n",
    "# Set training parameters\n",
    "minError = float('inf')\n",
    "bestRank = -1\n",
    "bestIteration = -1\n",
    "\n",
    "\n",
    "for rank in ranks:\n",
    "    \n",
    "    # Train the model using trainRDD, rank, seed, iterations and lambda value as model parameters\n",
    "    movieRecModel = ALS.train(trainRDD, \n",
    "                              rank = rank, \n",
    "                              seed = seed, \n",
    "                              iterations = iterations,\n",
    "                              lambda_ = lambdaRegParam)\n",
    "    \n",
    "    # Use the trained model to predict the ratings from validPredictionRDD using model.predictAll()\n",
    "    predictions = movieRecModel.predictAll(validPredictionRDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "    # Compare predicted ratings and actual ratings in validRDD\n",
    "    ratingsAndPreds = validRDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    \n",
    "    # Calculate RMSE error for the difference between ratings and predictions\n",
    "    error = math.sqrt(ratingsAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "    # save error into errors array\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    \n",
    "    print ('For rank %s the RMSE is %s' % (rank, error))\n",
    "    \n",
    "    # Check for best error and rank values\n",
    "    if error < minError:\n",
    "        minError = error\n",
    "        bestRank = rank\n",
    "\n",
    "print ('The best model was trained with rank %s' % bestRank)\n",
    "\n",
    "# For rank 2 the RMSE is 0.9420055149124977\n",
    "# For rank 4 the RMSE is 0.9336033590596436\n",
    "# For rank 6 the RMSE is 0.9481011551989256\n",
    "# For rank 8 the RMSE is 0.9447440477995621\n",
    "# For rank 10 the RMSE is 0.945496424548251\n",
    "# The best model was trained with rank 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Learning Phase\n",
    "\n",
    "Let's have a look at the predictions the model generated during last validation stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((86, 1084), 3.6499644385837957),\n",
       " ((242, 1084), 4.416651388654594),\n",
       " ((575, 1084), 4.077218760645106)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take 3 elements from the predictions RDD\n",
    "predictions.take(3)\n",
    "\n",
    "# [((86, 1084), 3.86742553170992),\n",
    "#  ((242, 1084), 4.535501401736902),\n",
    "#  ((575, 1084), 3.6610691865669773)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows we have the `((UserID,  MovieID), Rating)` tuple, similar to the ratings dataset. The `Ratings` field in the predictions RDD refer to the ratings predicted by the trained ALS model. \n",
    "\n",
    "Then we join these predictions with our validation data and the result looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 1061), (3.0, 2.0319217170239456)),\n",
       " ((1, 1129), (2.0, 1.5882013388411833)),\n",
       " ((2, 144), (3.0, 2.8069794140818063))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take 3 elements from the ratingsAndPredsRDD\n",
    "ratingsAndPreds.take(3)\n",
    "\n",
    "# [((1, 1061), (3.0, 2.699500607996266)),\n",
    "#  ((1, 1129), (2.0, 2.3323779150221435)),\n",
    "#  ((2, 144), (3.0, 2.7568272196178905))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output shows the format `((UserId, MovieId), Ratings, PredictedRatings)`. \n",
    "\n",
    "We then calculated the RMSE by taking the squred difference and calculating the mean value as our `error` value.\n",
    "\n",
    "### Testing the Model\n",
    "\n",
    "We shall now test the model with test dataset hich has been kept away from the learning phase upto this point. \n",
    "Use following parameters:\n",
    "* Use `trainRDD` for training the model.\n",
    "* Use `bestRank` value learnt during the validation phase.\n",
    "* Use other parameter values same as above. \n",
    "* Generate predictions with `testPredictionRDD`\n",
    "* Calculate error between predicted values and ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 0.9397494359602026\n"
     ]
    }
   ],
   "source": [
    "# Train and test the model with selected parameter bestRank\n",
    "\n",
    "movieRecModel = ALS.train(trainRDD, \n",
    "                           bestRank, \n",
    "                           seed=seed, \n",
    "                           iterations=iterations,\n",
    "                           lambda_=lambdaRegParam)\n",
    "\n",
    "# Calculate predictions for testPredictionRDD\n",
    "predictions = movieRecModel.predictAll(testPredictionRDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "# Combine real ratings and predictions\n",
    "ratingsAndPreds = testRDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "\n",
    "# Calculate RMSE\n",
    "error = math.sqrt(ratingsAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "print ('For testing data the RMSE is %s' % (error))\n",
    "\n",
    "# For testing data the RMSE is 0.9397494359602026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our testing error is very slightly higher than our validation error which is an expected behaviour. Due to probablistic nature of ALS algorithm, changing the seed value will also show somenfluctuations in these values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Complete MovieLens Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After identifying the optimum parameter values (ranks in our case), we will use the complete dataset to build our final model. Therefore, we need to process it the same way we did with the small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26024289 recommendations in the complete dataset\n"
     ]
    }
   ],
   "source": [
    "# Load the complete dataset ratings file (will be slow)\n",
    "MlRatingsFile = os.path.join(datasetsPath, 'ml-latest', 'ratings.csv')\n",
    "MlRatingsRaw= sc.textFile(MlRatingsFile)\n",
    "\n",
    "MlRatingsHeader = MlRatingsRaw.take(1)[0]\n",
    "\n",
    "# Parse the Ratings into the RDD\n",
    "MlRatingsRDD = MlRatingsRaw.filter(lambda line: line != MlRatingsHeader)\\\n",
    "                           .map(lambda line: line.split(\",\"))\\\n",
    "                           .map(lambda tokens: (int(tokens[0]),int(tokens[1]),float(tokens[2])))\\\n",
    "                           .cache()\n",
    "    \n",
    "print (\"There are %s recommendations in the complete dataset\" % (MlRatingsRDD.count()))\n",
    "\n",
    "# There are 26024289 recommendations in the complete dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a 70/30 train test split on `MlRatingsRDD` using `.randomsplit()` and train the model with complete ML datset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PErform a 70/30 train test split on full ratings RDD (be patient, this will be slow)\n",
    "trainRDD, testRDD = MlRatingsRDD.randomSplit([7, 3], seed=200)\n",
    "\n",
    "# Train the model with trainRDD\n",
    "MlModel = ALS.train(trainRDD, \n",
    "                    rank = bestRank, \n",
    "                    seed = seed, \n",
    "                    iterations = iterations, \n",
    "                    lambda_ = lambdaRegParam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform testing similar to the one shown above, using complete ML test dataset . \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 0.8360403934616045\n"
     ]
    }
   ],
   "source": [
    "# Use the complete trained model to predict with complete testRDD and calculate RMSE exactly as before. \n",
    "testPredictionRDD = testRDD.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "predictions = MlModel.predictAll(testPredictionRDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratingsAndPreds = testRDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "error = math.sqrt(ratingsAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "print ('For testing data the RMSE is %s' % (error))\n",
    "\n",
    "# For testing data the RMSE is 0.8360403934616045"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that we have much lower error than small dataset. The primary reason for this improvement is using large amounts of data for training the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: Making Recommendations\n",
    "\n",
    "When using collaborative filtering, getting recommendations is not as simple as predicting for the new entries using a previously generated model. Instead, we need to train again the model but including the new user preferences in order to compare them with other users in the dataset. That is, the recommender needs to be trained every time we have new user ratings (although a single model can be used by multiple users of course!). This makes the process expensive, and it is one of the reasons why scalability is a problem . Once we have our model trained, we can reuse it to obtain top recomendations for a given user or an individual rating for a particular movie. These are less costly operations than training the model itself.\n",
    "\n",
    "So let's first load the movies complete file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 45843 movies in the complete dataset\n"
     ]
    }
   ],
   "source": [
    "# Create completeMoviesRDD from complete ML movies dataset\n",
    "\n",
    "completeMoviesPath = os.path.join(datasetsPath, 'ml-latest', 'movies.csv')\n",
    "completeMoviesRaw = sc.textFile(completeMoviesPath)\n",
    "completeMoviesRawHeader= completeMoviesRaw.take(1)[0]\n",
    "\n",
    "# # Parse the Ratings into the RDD\n",
    "completeMoviesRDD = completeMoviesRaw.filter(lambda line: line != completeMoviesRawHeader)\\\n",
    "                                    .map(lambda line: line.split(\",\"))\\\n",
    "                                    .map(lambda tokens: (int(tokens[0]),tokens[1],tokens[2]))\\\n",
    "                                    .cache()\n",
    "\n",
    "# Get the list of all movie titles\n",
    "completeMoviesTitles = completeMoviesRDD.map(lambda x: (int(x[0]),x[1]))\n",
    "    \n",
    "print (\"There are %s movies in the complete dataset\" % (completeMoviesTitles.count()))\n",
    "\n",
    "# There are 45843 movies in the complete dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, We can also define a threshold ratings value to only include movies movies with a certain minimum number of ratings. For that, we need to count the number of ratings per movie. We can create a function that inputs an RDD and calculates the count and average ratings values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCountsAndAverages(IDRatings):\n",
    "    ratings = len(IDRatings[1])\n",
    "    return IDRatings[0], (ratings, float(sum(x for x in IDRatings[1]))/ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform following tasks in the given sequence: \n",
    "* use `MlRatingsRDD` to get movie ID and ratings values, and `groupby()` movie ID\n",
    "* Pass the new RDD to the function above to count and average rating value for all movies\n",
    "* create a new RDD `movieRatingsCountsRDD` to carry movie rating and count as a tuple\n",
    "* take 5 elements for inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(110, 66512), (858, 57070), (91542, 7196), (112552, 8455), (1210, 62714)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieIDRatingsRDD = (MlRatingsRDD.map(lambda x: (x[1], x[2])).groupByKey())\n",
    "movieIDAveRatingsRDD = movieIDRatingsRDD.map(getCountsAndAverages)\n",
    "movieRatingsCountsRDD = movieIDAveRatingsRDD .map(lambda x: (x[0], x[1][0]))\n",
    "\n",
    "movieRatingsCountsRDD.take(5)\n",
    "# [(110, 66512), (858, 57070), (91542, 7196), (112552, 8455), (1210, 62714)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding ratings by new user(s)\n",
    "\n",
    "Now we can create a new user and rate some movies under this user. We will put them in a new RDD and we will use the user ID 0, that is not assigned in the MovieLens dataset. Check the dataset movies file for ID to Tittle assignment (so you know what movies are you actually rating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New user ratings: [(0, 260, 4), (0, 1, 3), (0, 16, 3), (0, 25, 4), (0, 32, 4), (0, 335, 1), (0, 379, 1), (0, 296, 3), (0, 858, 5), (0, 50, 4)]\n"
     ]
    }
   ],
   "source": [
    "newUserID = 0\n",
    "\n",
    "# The format of each line is (userID, movieID, rating)\n",
    "newUserRating = [\n",
    "                    (0,260,4),  # Star Wars (1977)\n",
    "                    (0,1,3),    # Toy Story (1995)\n",
    "                    (0,16,3),   # Casino (1995)\n",
    "                    (0,25,4),   # Leaving Las Vegas (1995)\n",
    "                    (0,32,4),   # Twelve Monkeys (a.k.a. 12 Monkeys) (1995)\n",
    "                    (0,335,1),  # Flintstones, The (1994)\n",
    "                    (0,379,1),  # Timecop (1994)\n",
    "                    (0,296,3),  # Pulp Fiction (1994)\n",
    "                    (0,858,5) , # Godfather, The (1972)\n",
    "                    (0,50,4)    # Usual Suspects, The (1995)\n",
    "                   ]\n",
    "\n",
    "newUserRatingRDD = sc.parallelize(newUserRating)\n",
    "print ('New user ratings: %s' % newUserRatingRDD.take(10))\n",
    "\n",
    "# New user ratings: [(0, 260, 4), (0, 1, 3), (0, 16, 3), (0, 25, 4), (0, 32, 4), (0, 335, 1), (0, 379, 1), (0, 296, 3), (0, 858, 5), (0, 50, 4)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine the `newUserRatingRDD` with `completeMoviesRDD` using a `.union()` transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 110, 1.0), (1, 147, 4.5), (1, 858, 5.0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completeMlWithNewRating = MlRatingsRDD.union(newUserRatingRDD)\n",
    "completeMlWithNewRating.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the ALS model using all the parameters we selected before (when using the small dataset). Import time to also time the training process using puthon's `time()` function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model trained in 146.766 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import the time module to time the training process\n",
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "# Train the model with `completeMlWithNewRating` and parameters used earlier.\n",
    "\n",
    "new_ratings_model = ALS.train(completeMlWithNewRating, \n",
    "                              bestRank, \n",
    "                              seed=seed, \n",
    "                              iterations=iterations, \n",
    "                              lambda_= lambdaRegParam)\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"New model trained in %s seconds\" % round(tt,3))\n",
    "\n",
    "# New model trained in 146.766 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that the training process takes some time to finish. We will need to repeat that every time a user adds new ratings. Ideally we will do this in batches, and not for every single rating that comes into the system for every user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Top Recomendations\n",
    "\n",
    "After traning the model with new user ratings, we can get some recommendations. For that we will get an RDD with all the movies the new user hasn't rated yet. We will them together with the model to predict ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "newUserRatingsIds = map(lambda x: x[1], newUserRating) # get just movie IDs\n",
    "\n",
    "# keep just those not on the ID list \n",
    "newUserUnratedRDD = (completeMoviesRDD.filter(lambda x: x[0] not in newUserRatingsIds).map(lambda x: (newUserID, x[0])))\n",
    "\n",
    "# Use the input RDD, new_user_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies\n",
    "newRecRDD = new_ratings_model.predictAll(newUserUnratedRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform new_user_recommendations_RDD into pairs of the form (Movie ID, Predicted Rating)\n",
    "new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))\n",
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD)\n",
    "new_user_recommendations_rating_title_and_count_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]>=25).takeOrdered(25, key=lambda x: -x[1])\n",
    "\n",
    "print ('TOP recommended movies (with more than 25 reviews):\\n%s' %\n",
    "        '\\n'.join(map(str, top_movies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_movie = sc.parallelize([(0, 500)]) # Quiz Show (1994)\n",
    "individual_movie_rating_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)\n",
    "individual_movie_rating_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
