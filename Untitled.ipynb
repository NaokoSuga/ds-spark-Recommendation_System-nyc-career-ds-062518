{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Recommendation System in PySpark \n",
    "# Stage 1: Data Collection and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "* Demonsrate the basic understanding of how recommendation systems are used for personalization of services.\n",
    "* Download and store MovieLens data from within the python environment.\n",
    "* Parse and filter dataset, storing required features into RDDs. \n",
    "* Save the preprocessed dataset for later use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recommender/Recommendation Systems are one of the most successful applications of Machine Learning in the Big Data domain. Such systems are an integral part of the success of Amazon (Books, Items), Pandora/Spotify (Music), Google (News, Search), YouTube (Videos) etc.  For Amazon these systems bring more than 30% of revenues. For Netflix service, 75% of movies that people watch are from some sort of recommendation.\n",
    "\n",
    "> The goal of Recommendation Systems is to find what is likely to be of interest to the user. This enables organizations to offer a high level of personalization and customer tailored services.\n",
    "\n",
    "For online video content services like Netflix and Hulu, the need to build robust movie recommendation systems is extremely important. An example of recommendation system is such as this:\n",
    "\n",
    "    User A watches Game of Thrones and Breaking Bad.\n",
    "    User B performs a search query for Game of Thrones.\n",
    "    The system suggests Breaking Bad from data collected about user A.\n",
    "    \n",
    "\n",
    "This lab will guide you step-by-step into how to use the MovieLens dataset to build a movie recommendation system using collaborative filtering technique with Spark's Alternating Least Saqures implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie-lens Data Collection\n",
    "\n",
    "Social computing research centre at university of Minnesota, GroupLens Research,  has collected and made available rating related datasets at the [MovieLens website](http://movielens.org/). The datasets were collected over various periods of time and can be directly downloaded from [this location](http://grouplens.org/datasets/movielens/). \n",
    "\n",
    "A data dictionary with details on individual datasets and included features can be viewed [HERE](http://files.grouplens.org/datasets/movielens/ml-20m-README.html)\n",
    "\n",
    "For our experiment , we shall download the latest datasets direct from the website in the zip format.\n",
    "\n",
    "* **Small Dataset**: 100,000 ratings and 1,300 tag applications applied to 9,000 movies by 700 users. Last updated 10/2016.\n",
    "* **Complete Dataset**: 26,000,000 ratings and 750,000 tag applications applied to 45,000 movies by 270,000 users. Last updated 8/2017.\n",
    "\n",
    "Let's first the URLs for both datasets for downloading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallURL = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "completeURL = 'http://files.grouplens.org/datasets/movielens/ml-latest.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also import PySpark to our Python environment and and initiate a local SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]') # [*] represents a local context i.e. no cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Path \n",
    "\n",
    "We shall now define the download locations and filenames for both datasets. Let's create a folder \"datasets\" to download and save the zip files using [os.mkdir(path)](https://www.tutorialspoint.com/python/os_mkdir.htm). We also need to define paths for individual zip files for complete and small datasets. This can be achieved using [os.path.join()](https://code-maven.com/slides/python-programming/os-path-join) method. You need to import the `os` module into Python which provides directory system access, and save the zip files as **ml-latest.zip** and **ml-latest-small.zip**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "datasetsPath = \"datasets\"\n",
    "\n",
    "# create a directory \"datasets\"\n",
    "os.mkdir(datasetsPath)\n",
    "\n",
    "datasetsPath = os.path.join(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create path variables with filenames for both datasets\n",
    "completePath = os.path.join(datasetsPath, 'ml-latest.zip')\n",
    "smallPath = os.path.join(datasetsPath, 'ml-latest-small.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Datasets\n",
    "We can now move on and download both files from the server using URL provided above. \n",
    "\n",
    "Import `urllib.request` module in Python and use `urllib.request.urlretrieve(url, path)` with urls and paths given above for both datasets. A good resource on downloading files in python can be found [HERE](http://stackabuse.com/download-files-with-python/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "#Download the datasets, provide URL and local destination for each dataset\n",
    "small = urllib.request.urlretrieve(smallURL, smallPath)\n",
    "complete= urllib.request.urlretrieve(completeURL, completePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Datasets to Folders \n",
    "\n",
    "We need to unzip the small and full datasets into their own respective folders. Let's use Python's `zipfile` module to perform this task. Details on the `zipfile` module can be seen from [THIS](https://pymotw.com/2/zipfile/) resource.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(smallPath, \"r\") as z:\n",
    "    z.extractall(datasetsPath)\n",
    "\n",
    "with zipfile.ZipFile(completePath, \"r\") as z:\n",
    "    z.extractall(datasetsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Parsing, Selection and Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our SparkContext initialized, and datasets in their respective locations, we can now parse these csv files and read them into RDDs. Each dataset contains following files:  \n",
    "\n",
    "#### ratings.csv\n",
    "Each line in the ratings dataset (ratings.csv) is formatted as:**userId, movieId, rating, timestamp**\n",
    "\n",
    "#### movies.csv\n",
    "Each line in the movies (movies.csv) dataset is formatted as:**movieId, title, genres**\n",
    "\n",
    "Genres has the format:**Genre1|Genre2|Genre3...**\n",
    "\n",
    "#### tags.csv\n",
    "The tags file (tags.csv) has the format:**userId, movieId, tag, timestamp**\n",
    "\n",
    "#### links.csv\n",
    "Contains links to IMDB and has the format:**movieId, imdbId, tmdbId**\n",
    "\n",
    "The complete dataset also contains other files which are not needed for this experiment. \n",
    "\n",
    "The format of these files is uniform and simple and such comma delimited files can be parsed line by line using Python `split()`  once they are loaded into RDDs. \n",
    "\n",
    "Let's first work with the small dataset to get parsing code in place. We shall first parse ratings and movies files into two RDDs. We also need to filter out the header row  in each file. We shall only use ratings.csv and movies.csv for building a simple recommendation system. \n",
    "\n",
    ">For each line in the ratings dataset, we create a tuple of (UserID, MovieID, Rating). We drop the timestamp because we do not need it for this recommender.\n",
    "\n",
    ">For each line in the movies dataset, we create a tuple of (MovieID, Title). We drop the genres because we do not use them for this recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/ml-latest-small/ratings.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a path for identifying the ratings file \n",
    "\n",
    "smallRatingsPath = os.path.join(datasetsPath, 'ml-latest-small', 'ratings.csv')\n",
    "smallRatingsPath\n",
    "\n",
    "# 'datasets/ml-latest-small/ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'userId,movieId,rating,timestamp'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use .textFile() to read the raw contents of ratings file into an RDD\n",
    "# read the first line of this RDD as a header and view header contents\n",
    "\n",
    "smallRatingsRaw = sc.textFile(smallRatingsPath)\n",
    "smallRatingsHeader = smallRatingsRaw.take(1)[0]\n",
    "smallRatingsHeader\n",
    "\n",
    "# 'userId,movieId,rating,timestamp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can parse the raw data into a new RDD. Perform following transformations on `smallRatingsRaw`:\n",
    "\n",
    "1. Use `.filter()` to exclude the header information collected above\n",
    "2. Split each line of the csv file using `,` as the input argument with `split()` function\n",
    "3. Collect the first three elements of each row (UserID, MovieID, Rating) and discard timetsep field.\n",
    "4. Cache the final RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallRatingsNoHeader= smallRatingsRaw.filter(lambda line: line != smallRatingsHeader )\n",
    "smallRatingsSplit = smallRatingsNoHeader.map(lambda line: line.split(\",\"))\n",
    "smallRatingsRDD = smallRatingsSplit.map(lambda tokens: (tokens[0],tokens[1],tokens[2]))\n",
    "smallRatingsRDD.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '31', '2.5'),\n",
       " ('1', '1029', '3.0'),\n",
       " ('1', '1061', '3.0'),\n",
       " ('1', '1129', '2.0'),\n",
       " ('1', '1172', '4.0')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first five rows of the final RDD\n",
    "smallRatingsRDD.take(5)\n",
    "\n",
    "# [('1', '31', '2.5'),\n",
    "#  ('1', '1029', '3.0'),\n",
    "#  ('1', '1061', '3.0'),\n",
    "#  ('1', '1129', '2.0'),\n",
    "#  ('1', '1172', '4.0')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now proceed in a similar way with movies.csv file. Repeat following steps as performed above:\n",
    "1. Create a path variable identifying the location of movies.csv\n",
    "2. Read the text file into RDD\n",
    "3. Exclude the header information\n",
    "4. Split the line contents of the csv file\n",
    "5. Read the contents of resulting RDD creating a (MovieID, Title) tuple and discard genres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'Toy Story (1995)'),\n",
       " ('2', 'Jumanji (1995)'),\n",
       " ('3', 'Grumpier Old Men (1995)'),\n",
       " ('4', 'Waiting to Exhale (1995)'),\n",
       " ('5', 'Father of the Bride Part II (1995)')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallMoviesPath = os.path.join(datasetsPath, 'ml-latest-small', 'movies.csv')\n",
    "\n",
    "smallMoviesRaw = sc.textFile(smallMoviesPath)\n",
    "smallMoviesHeader = smallMoviesRaw.take(1)[0]\n",
    "\n",
    "smallMoviesRDD = smallMoviesRaw.filter(lambda line: line != smallMoviesHeader)\\\n",
    "                               .map(lambda line: line.split(\",\"))\\\n",
    "                               .map(lambda tokens: (tokens[0],tokens[1]))\\\n",
    "                               .cache()\n",
    "    \n",
    "smallMoviesRDD.take(5)\n",
    "\n",
    "# [('1', 'Toy Story (1995)'),\n",
    "#  ('2', 'Jumanji (1995)'),\n",
    "#  ('3', 'Grumpier Old Men (1995)'),\n",
    "#  ('4', 'Waiting to Exhale (1995)'),\n",
    "#  ('5', 'Father of the Bride Part II (1995)')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Pre-Processed Data (optional)\n",
    "\n",
    "We can now save our preprocessed datasets. Create a folder \"processed\" and save smallMovieRDD and smallRatingsRDD using `RDD.saveAsTExtFile(location, filename)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory \"processed\" and store the preprocessed dataset RDDs as text files using .saveAsTExtFiles() method. \n",
    "\n",
    "processedPath = 'processed'\n",
    "os.mkdir(processedPath)\n",
    "\n",
    "smallMoviesRDD.saveAsTextFile(os.path.join(processedPath, 'smallMoviesRDD'))\n",
    "smallRatingsRDD.saveAsTextFile(os.path.join(processedPath, 'smallRatingsRDD'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMARY SECTION 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we perfomed data collection with some filtering to download the MovieLens dataset from the internet and storing these in their respective folders from within the python environment. We also looked at some basic filtering steps for feature selection for the movie recommendation system. This lab concludes by saving preprocessed dataset into text files, which will be used for training and evaluation in later labs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Alternate Least Squares: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data as Testing , Training and Validation Sets. \n",
    "\n",
    "We can now go ahead and split the data from training the recommendation system. We can use spark's `randomsplit()` transformation that uses given weights to split an rdd into any number of sub-RDDs. The standared usage of this transformation function is :\n",
    "> `RDD.randomSplit(weights, seed=None)`\n",
    "\n",
    "where:\n",
    "weights – weights for splits, will be normalized if they don’t sum to 1\n",
    "\n",
    "seed – random seed\n",
    "\n",
    "Remember that the exact number of entries in each dataset varies slightly due to the random nature of the randomSplit() transformation.\n",
    "\n",
    "Let's split the smallRatingsRDD into training, testing and validation RDDs using weights 6,2,2 respectively. Perform a `.count` on resulting datasets to view the count of each RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59907, 19993, 20104)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split ratingsRDD into training, validation and testing RDDs.\n",
    "trainRDD, validRDD, testRDD = smallRatingsRDD.randomSplit([6, 2, 2], seed=100)\n",
    "\n",
    "trainRDD.count(), testRDD.count(), validRDD.count()\n",
    "\n",
    "# (59902, 19991, 20111) > trainRDD, validRDD ,testRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prediction, we need customerID and movieID from validation and test RDDs respectively. Let's map these values into two new RDDs which will be used for training and validation purpose. We shall ignore the actual ratings values for these RDDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('1', '31'), ('1', '1061'), ('1', '1129')],\n",
       " [('1', '1287'), ('1', '1293'), ('1', '1339')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validPredictionRDD = validRDD.map(lambda x: (x[0], x[1]))\n",
    "testPredictionRDD = testRDD.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "validPredictionRDD.take(3), testPredictionRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering\n",
    "\n",
    "Collaborative filtering allows us to make predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). \n",
    "\n",
    "The underlying assumption is that if a user A has the same opinion as a user B on an issue, A is more likely to have B's opinion on a different issue x than to have the opinion on x of a user chosen randomly. Following gif (from [Wikipedia](https://en.wikipedia.org/wiki/Collaborative_filtering)) shows an example of collaborative filtering. At first, people rate different items (like videos, images, games). Then, the system makes predictions about a user's rating for an item not rated yet. The new predictions are built upon the existing ratings of other users with similar ratings with the active user. In the image, the system predicts that the user will not like the video.\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/5/52/Collaborative_filtering.gif)\n",
    "\n",
    "\n",
    "Spark MLlib library for Machine Learning provides a Collaborative Filtering implementation by using Alternating Least Squares. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Least Squares in Spark\n",
    "\n",
    "The ALS implementation for collaborative filtering in MLlib has the following parameters:\n",
    "\n",
    "* `numBlocks` is the number of blocks used to parallelize computation **(set to -1 to auto-configure)**\n",
    "* `rank` is the number of latent factors in the model. **(use the list [2,4,6,8,10] as rank values)**\n",
    "* `iterations` is the number of iterations to run. **(set to 15)**\n",
    "* `lambda` specifies the regularization parameter in ALS.**(set to 0.1)**\n",
    "* `implicitPrefs` specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data.**(use default)**\n",
    "* `alpha` is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations.**(use default)**\n",
    "\n",
    "Details on spark's ALS implementation can be viewed [HERE](https://spark.apache.org/docs/2.2.0/mllib-collaborative-filtering.html)\n",
    "\n",
    "\n",
    "\n",
    "We shall now import the ALS algorithm rom spark's machine learning library `mllib` and set the learning and training parameter values as shown above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Validation for hyper-parameter optimization\n",
    "\n",
    "We can now start our training process using above parameter values which would include following steps: \n",
    "\n",
    "* Run the training for each of the rank values in our `ranks` list inside a for loop.\n",
    "* Train the model using trainRDD, rank, seed, iterations and lambda value as model parameters. \n",
    "* Validate the trained model by predicting ratings for `validPredictionRDD`.\n",
    "* Compare predicted ratings to actual ratings. \n",
    "* calculate error as RMSE for each rank. \n",
    "\n",
    "For sake of simplicity, we shall repeat training process for changing ranks value **only**. Other values can also be changed as a detailed grid search for improved predictive performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 2 the RMSE is 0.9373037986511492\n",
      "For rank 4 the RMSE is 0.9379199229901556\n",
      "For rank 6 the RMSE is 0.937083572495462\n",
      "For rank 8 the RMSE is 0.9418894910915557\n",
      "For rank 10 the RMSE is 0.9476290426992048\n",
      "The best model was trained with rank 6\n"
     ]
    }
   ],
   "source": [
    "# Import ALS from spark's mllib\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "\n",
    "# set learning parameters \n",
    "seed = 20\n",
    "iterations = 15\n",
    "lambdaRegParam = 0.1\n",
    "ranks = [2, 4, 6, 8, 10]\n",
    "errors = [0, 0, 0, 0, 0] # initialize a matrix for storing error values\n",
    "err = 0 # iterator for above list \n",
    "#tolerance = 0.02\n",
    "\n",
    "# Set training parameters\n",
    "minError = float('inf')\n",
    "bestRank = -1\n",
    "bestIteration = -1\n",
    "\n",
    "\n",
    "for rank in ranks:\n",
    "    \n",
    "    # Train the model using trainRDD, rank, seed, iterations and lambda value as model parameters\n",
    "    movieRecModel = ALS.train(trainRDD, \n",
    "                      rank = rank, \n",
    "                      seed = seed, \n",
    "                      iterations = iterations,\n",
    "                      lambda_ = lambdaRegParam)\n",
    "    \n",
    "    # Use the trained model to predict the ratings from validPredictionRDD using model.predictAll()\n",
    "    predictions = movieRecModel.predictAll(validPredictionRDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "    # Compare predicted ratings and actual ratings in validRDD\n",
    "    ratingsAndPreds = validRDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    \n",
    "    # Calculate RMSE error for the difference between ratings and predictions\n",
    "    error = math.sqrt(ratingsAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "    # save error into errors array\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    \n",
    "    print ('For rank %s the RMSE is %s' % (rank, error))\n",
    "    \n",
    "    # Check for best error and rank values\n",
    "    if error < minError:\n",
    "        minError = error\n",
    "        bestRank = rank\n",
    "\n",
    "print ('The best model was trained with rank %s' % bestRank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Learning Phase\n",
    "\n",
    "Let's have a look at the predictions the model generated during last validation stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((86, 1084), 3.86742553170992),\n",
       " ((242, 1084), 4.535501401736902),\n",
       " ((575, 1084), 3.6610691865669773)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take 3 elements from the predictions RDD\n",
    "predictions.take(3)\n",
    "\n",
    "# [((86, 1084), 3.86742553170992),\n",
    "#  ((242, 1084), 4.535501401736902),\n",
    "#  ((575, 1084), 3.6610691865669773)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows we have the `((UserID,  MovieID), Rating)` tuple, similar to the ratings dataset. The `Ratings` field in the predictions RDD refer to the ratings predicted by the trained ALS model. \n",
    "\n",
    "Then we join these predictions with our validation data and the result looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 1061), (3.0, 2.699500607996266)),\n",
       " ((1, 1129), (2.0, 2.3323779150221435)),\n",
       " ((2, 144), (3.0, 2.7568272196178905))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take 3 elements from the ratingsAndPredsRDD\n",
    "ratingsAndPreds.take(3)\n",
    "\n",
    "# [((1, 1061), (3.0, 2.699500607996266)),\n",
    "#  ((1, 1129), (2.0, 2.3323779150221435)),\n",
    "#  ((2, 144), (3.0, 2.7568272196178905))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output shows the format `((UserId, MovieId), Ratings, PredictedRatings)`. \n",
    "\n",
    "We then calculated the RMSE by taking the squred difference and calculating the mean value as our `error` value.\n",
    "\n",
    "### Testing the Model\n",
    "\n",
    "We shall now test the model with test dataset hich has been kept away from the learning phase upto this point. \n",
    "Use following parameters:\n",
    "* USe `trainRDD` for training the model.\n",
    "* Use `bestRank` value learnt during the validation phase.\n",
    "* Use other parameter values same as above. \n",
    "* Generate predictions with `testPredictionRDD`\n",
    "* Calculate error between predicted values and ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 0.9477724241906258\n"
     ]
    }
   ],
   "source": [
    "# Train and test the model with selected parameter bestRank\n",
    "\n",
    "movieRecModel = ALS.train(trainRDD, \n",
    "                           bestRank, \n",
    "                           seed=seed, \n",
    "                           iterations=iterations,\n",
    "                           lambda_=lambdaRegParam)\n",
    "\n",
    "# Calculate predictions for testPredictionRDD\n",
    "predictions = movieRecModel.predictAll(testPredictionRDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "# Combine real ratings and predictions\n",
    "ratingsAndPreds = testRDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "\n",
    "# Calculate RMSE\n",
    "error = math.sqrt(ratingsAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "print ('For testing data the RMSE is %s' % (error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our testing error is slightly higher than our validation error which is an expected behaviour. Due to probablistic nature of ALS algorithm, changing the seed value will also show somefluctuations in these values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Complete MovieLens Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After identifying the optimum parameter values (ranks in our case), we will use the complete dataset to build our final model. Therefore, we need to process it the same way we did with the small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26024289 recommendations in the complete dataset\n"
     ]
    }
   ],
   "source": [
    "# Load the complete dataset ratings file (will be slow)\n",
    "MlRatingsFile = os.path.join(datasetsPath, 'ml-latest', 'ratings.csv')\n",
    "MlRatingsRaw= sc.textFile(MlRatingsFile)\n",
    "\n",
    "MlRatingsHeader = MlRatingsRaw.take(1)[0]\n",
    "\n",
    "# Parse the Ratings into the RDD\n",
    "MlRatingsRDD = MlRatingsRaw.filter(lambda line: line != MlRatingsHeader)\\\n",
    "                           .map(lambda line: line.split(\",\"))\\\n",
    "                           .map(lambda tokens: (int(tokens[0]),int(tokens[1]),float(tokens[2])))\\\n",
    "                           .cache()\n",
    "    \n",
    "print (\"There are %s recommendations in the complete dataset\" % (MlRatingsRDD.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a 70/30 train test split on `MlRatingsRDD` using `.randomsplit()` and train the model with complete ML datset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PErform a 70/30 train test split on full ratings RDD\n",
    "trainRDD, testRDD = MlRatingsRDD.randomSplit([7, 3], seed=200)\n",
    "\n",
    "# Train the model with trainRDD\n",
    "MlModel = ALS.train(trainRDD, \n",
    "                    rank = bestRank, \n",
    "                    seed = seed, \n",
    "                    iterations = iterations, \n",
    "                    lambda_ = lambdaRegParam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform testing similar to the one shown above, using complete ML test dataset . \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 0.8213246944974424\n"
     ]
    }
   ],
   "source": [
    "# Use the complete trained model to predict with complete testRDD and calculate RMSE exactly as before. \n",
    "testPredictionRDD = testRDD.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "predictions = MlModel.predictAll(testPredictionRDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratingsAndPreds = testRDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "error = math.sqrt(ratingsAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "print ('For testing data the RMSE is %s' % (error))\n",
    "\n",
    "# For testing data the RMSE is 0.8213246944974424"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that we have much higher accuracy than small dataset. The primary reason for this improvement is using large amounts of data for training the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: Making Recommendations\n",
    "\n",
    "When using collaborative filtering, getting recommendations is not as simple as predicting for the new entries using a previously generated model. Instead, we need to train again the model but including the new user preferences in order to compare them with other users in the dataset. That is, the recommender needs to be trained every time we have new user ratings (although a single model can be used by multiple users of course!). This makes the process expensive, and it is one of the reasons why scalability is a problem (and Spark a solution!). Once we have our model trained, we can reuse it to obtain top recomendations for a given user or an individual rating for a particular movie. These are less costly operations than training the model itself.\n",
    "\n",
    "So let's first load the movies complete file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_movies_file = os.path.join(datasets_path, 'ml-latest', 'movies.csv')\n",
    "complete_movies_raw_data = sc.textFile(complete_movies_file)\n",
    "complete_movies_raw_data_header = complete_movies_raw_data.take(1)[0]\n",
    "\n",
    "# Parse\n",
    "complete_movies_data = complete_movies_raw_data.filter(lambda line: line!=complete_movies_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),tokens[1],tokens[2])).cache()\n",
    "\n",
    "complete_movies_titles = complete_movies_data.map(lambda x: (int(x[0]),x[1]))\n",
    "    \n",
    "print (\"There are %s movies in the complete dataset\" % (complete_movies_titles.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts_and_averages(ID_and_ratings_tuple):\n",
    "    nratings = len(ID_and_ratings_tuple[1])\n",
    "    return ID_and_ratings_tuple[0], (nratings, float(sum(x for x in ID_and_ratings_tuple[1]))/nratings)\n",
    "\n",
    "movie_ID_with_ratings_RDD = (complete_ratings_data.map(lambda x: (x[1], x[2])).groupByKey())\n",
    "movie_ID_with_avg_ratings_RDD = movie_ID_with_ratings_RDD.map(get_counts_and_averages)\n",
    "movie_rating_counts_RDD = movie_ID_with_avg_ratings_RDD.map(lambda x: (x[0], x[1][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_ID = 0\n",
    "\n",
    "# The format of each line is (userID, movieID, rating)\n",
    "\n",
    "new_user_ratings = [\n",
    "     (0,260,4), # Star Wars (1977)\n",
    "     (0,1,3), # Toy Story (1995)\n",
    "     (0,16,3), # Casino (1995)\n",
    "     (0,25,4), # Leaving Las Vegas (1995)\n",
    "     (0,32,4), # Twelve Monkeys (a.k.a. 12 Monkeys) (1995)\n",
    "     (0,335,1), # Flintstones, The (1994)\n",
    "     (0,379,1), # Timecop (1994)\n",
    "     (0,296,3), # Pulp Fiction (1994)\n",
    "     (0,858,5) , # Godfather, The (1972)\n",
    "     (0,50,4) # Usual Suspects, The (1995)\n",
    "    ]\n",
    "new_user_ratings_RDD = sc.parallelize(new_user_ratings)\n",
    "print ('New user ratings: %s' % new_user_ratings_RDD.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data_with_new_ratings_RDD = complete_ratings_data.union(new_user_ratings_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "new_ratings_model = ALS.train(complete_data_with_new_ratings_RDD, best_rank, seed=seed, \n",
    "                              iterations=iterations, lambda_=regularization_parameter)\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"New model trained in %s seconds\" % round(tt,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_ratings_ids = map(lambda x: x[1], new_user_ratings) # get just movie IDs\n",
    "# keep just those not on the ID list \n",
    "new_user_unrated_movies_RDD = (complete_movies_data.filter(lambda x: x[0] not in new_user_ratings_ids).map(lambda x: (new_user_ID, x[0])))\n",
    "\n",
    "# Use the input RDD, new_user_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies\n",
    "new_user_recommendations_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform new_user_recommendations_RDD into pairs of the form (Movie ID, Predicted Rating)\n",
    "new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))\n",
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD)\n",
    "new_user_recommendations_rating_title_and_count_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]>=25).takeOrdered(25, key=lambda x: -x[1])\n",
    "\n",
    "print ('TOP recommended movies (with more than 25 reviews):\\n%s' %\n",
    "        '\\n'.join(map(str, top_movies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_movie = sc.parallelize([(0, 500)]) # Quiz Show (1994)\n",
    "individual_movie_rating_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)\n",
    "individual_movie_rating_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
